{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PrivSyn Tabular Documentation \u00b6 Welcome to the MkDocs-powered handbook for the PrivSyn tabular synthesis project. Use the navigation on the left to jump straight into frontend internals, backend APIs, testing strategy, deployment notes, and privacy considerations. PrivSyn\u2019s synthesizer logic builds on two peer-reviewed systems: PrivSyn (Zhang et al., USENIX Security 2021) \u2013 https://www.usenix.org/system/files/sec21-zhang-zhikun.pdf AIM (McKenna et al., PVLDB 2022) \u2013 https://www.vldb.org/pvldb/vol15/p2599-mckenna.pdf Quick Links \u00b6 Repository README \u2013 high-level overview, quick-start commands, and architecture diagrams. Frontend Guide \u2013 React/Vite structure, metadata editors, and component testing. Backend Guide \u2013 FastAPI endpoints, synthesis orchestration, and data flow. Testing Playbook \u2013 pytest conventions, Playwright E2E flows, and focused command examples. Deployment Checklist \u2013 Docker build, Cloud Run notes, and prod env variables. Privacy Notes \u2013 epsilon/delta guidance and rho-CDP budgeting for PrivSyn vs. AIM. Getting Started Locally \u00b6 pip install mkdocs mkdocs serve Browse the site at http://127.0.0.1:8000/ to view these pages with navigation search and table-of-contents support. Contributing \u00b6 Keep project documentation under the docs/ directory. When you add a new Markdown file, update mkdocs.yml so it appears in the navigation, and run mkdocs serve to verify formatting locally.","title":"Overview"},{"location":"#privsyn-tabular-documentation","text":"Welcome to the MkDocs-powered handbook for the PrivSyn tabular synthesis project. Use the navigation on the left to jump straight into frontend internals, backend APIs, testing strategy, deployment notes, and privacy considerations. PrivSyn\u2019s synthesizer logic builds on two peer-reviewed systems: PrivSyn (Zhang et al., USENIX Security 2021) \u2013 https://www.usenix.org/system/files/sec21-zhang-zhikun.pdf AIM (McKenna et al., PVLDB 2022) \u2013 https://www.vldb.org/pvldb/vol15/p2599-mckenna.pdf","title":"PrivSyn Tabular Documentation"},{"location":"#quick-links","text":"Repository README \u2013 high-level overview, quick-start commands, and architecture diagrams. Frontend Guide \u2013 React/Vite structure, metadata editors, and component testing. Backend Guide \u2013 FastAPI endpoints, synthesis orchestration, and data flow. Testing Playbook \u2013 pytest conventions, Playwright E2E flows, and focused command examples. Deployment Checklist \u2013 Docker build, Cloud Run notes, and prod env variables. Privacy Notes \u2013 epsilon/delta guidance and rho-CDP budgeting for PrivSyn vs. AIM.","title":"Quick Links"},{"location":"#getting-started-locally","text":"pip install mkdocs mkdocs serve Browse the site at http://127.0.0.1:8000/ to view these pages with navigation search and table-of-contents support.","title":"Getting Started Locally"},{"location":"#contributing","text":"Keep project documentation under the docs/ directory. When you add a new Markdown file, update mkdocs.yml so it appears in the navigation, and run mkdocs serve to verify formatting locally.","title":"Contributing"},{"location":"backend/","text":"Backend Guide \u00b6 Architecture Overview \u00b6 FastAPI application : web_app/main.py /synthesize infers metadata and caches the original DataFrame plus draft domain info. /confirm_synthesis reconstructs the DataFrame with user overrides and invokes the synthesizer. /download_synthesized_data/{session_id} streams the generated CSV for a confirmed synthesis session. /evaluate calculates metadata-aware metrics using web_app/data_comparison.py (keyed by the same session ID). Synthesis service : web_app/synthesis_service.py Bridges the cached inference bundle to the selected synthesizer. Handles preprocessing (clipping, binning, categorical remap) before handing off to PrivSyn or AIM. Algorithm references: PrivSyn follows the approach in PrivSyn: Differentially Private Data Synthesis ; the AIM adapter implements The AIM Mechanism for Differentially Private Synthetic Data . Key Modules \u00b6 Module Role web_app/data_inference.py Detect column types, normalise metadata, and prepare draft domain/info payloads. web_app/synthesis_service.py Applies overrides, constructs the preprocesser, runs the synthesizer, and persists outputs. web_app/data_comparison.py Implements histogram-aware TVD and other metrics for evaluation. method/synthesis/privsyn/privsyn.py PrivSyn implementation (marginal selection + GUM). method/api/base.py Core synthesizer API ( SynthRegistry , PrivacySpec , RunConfig , Synthesizer protocol). method/api/utils.py Helper utilities used by adapters (e.g., split_df_by_type , schema enforcement). method/synthesis/AIM/adapter.py Adapter wiring AIM into the unified interface provided by method/api . method/preprocess_common/ Shared discretizers (PrivTree, DAWA) and helper utilities. Unified Synthesis Interface \u00b6 method/api/base.py defines the shared contract every synthesis method must follow: SynthRegistry exposes register , get , and list helpers so adapters (e.g., method/synthesis/privsyn/__init__.py , method/synthesis/AIM/__init__.py ) can self-register at import time. PrivacySpec and RunConfig capture the caller\u2019s DP/compute requirements and are passed through to each adapter. _AdapterSynth and _AdapterFitted wrap legacy prepare/run functions so existing method code needs minimal changes. The backend dispatcher ( web_app/methods_dispatcher.py ) and tests such as test/test_methods_dispatcher.py rely on this registry to treat every method uniformly. Method-specific modules ( method/synthesis/<name>/native.py , config.py , parameter_parser.py , etc.) stay alongside each algorithm because they encode behaviour that other methods do not share (e.g., PrivSyn\u2019s marginal-selection parameters or AIM\u2019s workload configuration). Keep the registry small and general, and let each method own its internal configuration files. Endpoint Notes \u00b6 POST /synthesize \u00b6 Expects multipart form (fields documented in test/test_api_contract.py ). For sample runs, omit the file and set dataset_name=adult . Stores the uploaded DataFrame and inferred metadata under a temporary UUID in memory. All columns from the uploaded table participate in metadata inference; the API no longer accepts or drops a distinct target column. POST /confirm_synthesis \u00b6 Requires the unique_id returned by /synthesize . Accepts JSON strings for confirmed_domain_data and confirmed_info_data . Runs the chosen synthesizer ( privsyn or aim ) and writes synthesized CSV + evaluation bundle to the temp directory. GET /download_synthesized_data/{session_id} \u00b6 Streams the generated CSV for a previously confirmed synthesis session. Backed by an in-memory SessionStore keyed by the unique_id returned from /synthesize . POST /evaluate \u00b6 Accepts session_id (form field) and reuses cached original/synth data to compute metrics (e.g., histogram TVD for numeric columns). Local Development \u00b6 uvicorn web_app.main:app --reload --port 8001 # Optionally set VITE_API_BASE_URL when running the frontend separately export VITE_API_BASE_URL=http://127.0.0.1:8001 Configuration Tips \u00b6 CORS origins are defined in web_app/main.py . Update the allow_origins list to include any new frontend domains. Set the ADDITIONAL_CORS_ORIGINS environment variable (comma-separated list) in production to append extra origins\u2014useful for Vercel preview/prod URLs. Temporary artifacts (original data, synthesized CSVs) land under temp_synthesis_output/ . Keep an eye on disk usage during iterative testing. Use environmental overrides or .env files for production secrets (database URLs, etc.)\u2014the current setup only handles the stateless demo flow.","title":"Backend"},{"location":"backend/#backend-guide","text":"","title":"Backend Guide"},{"location":"backend/#architecture-overview","text":"FastAPI application : web_app/main.py /synthesize infers metadata and caches the original DataFrame plus draft domain info. /confirm_synthesis reconstructs the DataFrame with user overrides and invokes the synthesizer. /download_synthesized_data/{session_id} streams the generated CSV for a confirmed synthesis session. /evaluate calculates metadata-aware metrics using web_app/data_comparison.py (keyed by the same session ID). Synthesis service : web_app/synthesis_service.py Bridges the cached inference bundle to the selected synthesizer. Handles preprocessing (clipping, binning, categorical remap) before handing off to PrivSyn or AIM. Algorithm references: PrivSyn follows the approach in PrivSyn: Differentially Private Data Synthesis ; the AIM adapter implements The AIM Mechanism for Differentially Private Synthetic Data .","title":"Architecture Overview"},{"location":"backend/#key-modules","text":"Module Role web_app/data_inference.py Detect column types, normalise metadata, and prepare draft domain/info payloads. web_app/synthesis_service.py Applies overrides, constructs the preprocesser, runs the synthesizer, and persists outputs. web_app/data_comparison.py Implements histogram-aware TVD and other metrics for evaluation. method/synthesis/privsyn/privsyn.py PrivSyn implementation (marginal selection + GUM). method/api/base.py Core synthesizer API ( SynthRegistry , PrivacySpec , RunConfig , Synthesizer protocol). method/api/utils.py Helper utilities used by adapters (e.g., split_df_by_type , schema enforcement). method/synthesis/AIM/adapter.py Adapter wiring AIM into the unified interface provided by method/api . method/preprocess_common/ Shared discretizers (PrivTree, DAWA) and helper utilities.","title":"Key Modules"},{"location":"backend/#unified-synthesis-interface","text":"method/api/base.py defines the shared contract every synthesis method must follow: SynthRegistry exposes register , get , and list helpers so adapters (e.g., method/synthesis/privsyn/__init__.py , method/synthesis/AIM/__init__.py ) can self-register at import time. PrivacySpec and RunConfig capture the caller\u2019s DP/compute requirements and are passed through to each adapter. _AdapterSynth and _AdapterFitted wrap legacy prepare/run functions so existing method code needs minimal changes. The backend dispatcher ( web_app/methods_dispatcher.py ) and tests such as test/test_methods_dispatcher.py rely on this registry to treat every method uniformly. Method-specific modules ( method/synthesis/<name>/native.py , config.py , parameter_parser.py , etc.) stay alongside each algorithm because they encode behaviour that other methods do not share (e.g., PrivSyn\u2019s marginal-selection parameters or AIM\u2019s workload configuration). Keep the registry small and general, and let each method own its internal configuration files.","title":"Unified Synthesis Interface"},{"location":"backend/#endpoint-notes","text":"","title":"Endpoint Notes"},{"location":"backend/#post-synthesize","text":"Expects multipart form (fields documented in test/test_api_contract.py ). For sample runs, omit the file and set dataset_name=adult . Stores the uploaded DataFrame and inferred metadata under a temporary UUID in memory. All columns from the uploaded table participate in metadata inference; the API no longer accepts or drops a distinct target column.","title":"POST /synthesize"},{"location":"backend/#post-confirm_synthesis","text":"Requires the unique_id returned by /synthesize . Accepts JSON strings for confirmed_domain_data and confirmed_info_data . Runs the chosen synthesizer ( privsyn or aim ) and writes synthesized CSV + evaluation bundle to the temp directory.","title":"POST /confirm_synthesis"},{"location":"backend/#get-download_synthesized_datasession_id","text":"Streams the generated CSV for a previously confirmed synthesis session. Backed by an in-memory SessionStore keyed by the unique_id returned from /synthesize .","title":"GET /download_synthesized_data/{session_id}"},{"location":"backend/#post-evaluate","text":"Accepts session_id (form field) and reuses cached original/synth data to compute metrics (e.g., histogram TVD for numeric columns).","title":"POST /evaluate"},{"location":"backend/#local-development","text":"uvicorn web_app.main:app --reload --port 8001 # Optionally set VITE_API_BASE_URL when running the frontend separately export VITE_API_BASE_URL=http://127.0.0.1:8001","title":"Local Development"},{"location":"backend/#configuration-tips","text":"CORS origins are defined in web_app/main.py . Update the allow_origins list to include any new frontend domains. Set the ADDITIONAL_CORS_ORIGINS environment variable (comma-separated list) in production to append extra origins\u2014useful for Vercel preview/prod URLs. Temporary artifacts (original data, synthesized CSVs) land under temp_synthesis_output/ . Keep an eye on disk usage during iterative testing. Use environmental overrides or .env files for production secrets (database URLs, etc.)\u2014the current setup only handles the stateless demo flow.","title":"Configuration Tips"},{"location":"deployment/","text":"Deployment Guide \u00b6 This document outlines the most common deployment paths for the PrivSyn web application\u2014local Docker usage, Google Cloud Run, and Vercel (frontend). It highlights the environment variables you need to set and the expected directory layout. 1. Local Docker Image \u00b6 Build the image: bash docker build -t privsyn-tabular . Run the container, exposing the FastAPI port: bash docker run --rm -p 8080:8080 \\ -e VITE_API_BASE_URL=\"http://localhost:8080\" \\ privsyn-tabular The backend listens on $PORT (defaults to 8080 for Cloud Run compatibility). The Dockerfile bundles the built frontend under web_app/static/ , so no additional proxy is required. 2. Google Cloud Run \u00b6 Build and push the container: bash gcloud builds submit --tag gcr.io/<PROJECT_ID>/privsyn Deploy to Cloud Run: bash gcloud run deploy privsyn \\ --image gcr.io/<PROJECT_ID>/privsyn \\ --platform managed \\ --allow-unauthenticated \\ --set-env-vars=\"VITE_API_BASE_URL=https://<service-url>\" Configure Cloud Run service variables: VITE_API_BASE_URL : the public URL of the service if you intend to serve the frontend from the same container. CORS_ALLOW_ORIGINS : optional comma-separated list of additional origins to append to the defaults in web_app/main.py . Free tier constraints: the Cloud Run free tier grants limited CPU, RAM, and request duration. Large uploads or AIM runs frequently exceed those caps, leading to timeouts or OOM restarts. For heavy workloads, pull the code locally (or to a beefier VM) and run the backend outside Cloud Run. 2.1 Continuous deployment from GitHub Actions \u00b6 Automated deployments run via .github/workflows/deploy-cloudrun.yml whenever main is updated. The workflow: Checks out the repository. Authenticates to GCP using GCP_SA_KEY (a JSON service-account key stored as a GitHub secret). Builds and pushes gcr.io/gen-lang-client-0649776758/privsyn-tabular via gcloud builds submit . Deploys the image to Cloud Run in us-east4 with --allow-unauthenticated . To keep it working you must ensure the following GitHub secrets are defined: Secret Value GCP_SA_KEY Service-account JSON with roles Cloud Run Admin , Cloud Build Editor , Service Account User . The project ( gen-lang-client-0649776758 ), region ( us-east4 ), and image name are baked into the workflow. If you need a different target, update .github/workflows/deploy-cloudrun.yml . 3. Vercel Frontend + Hosted Backend \u00b6 Deploy the backend (Docker, Cloud Run, or elsewhere) and note the public base URL. In the Vercel project (or other static hosting provider): Set VITE_API_BASE_URL to the backend URL. Run npm run build to produce frontend/dist . Serve the built assets or configure Vercel to use the static output directory. Update allow_origins in web_app/main.py to include the Vercel domain. 4. Environment Variables \u00b6 Variable Purpose Default VITE_API_BASE_URL Frontend \u2192 backend URL. http://127.0.0.1:8001 CORS_ALLOW_ORIGINS Optional extra origins; comma separated. \u2014 PORT FastAPI listening port (Cloud Run). 8080 LOG_LEVEL Optional override for FastAPI logging level. INFO 5. Storage & Sessions \u00b6 Temporary artifacts (uploaded parquet files, synthesized CSVs) are stored under temp_synthesis_output/runs/{session_id} . Sessions expire automatically after six hours. For production, consider pointing the session store to Redis or another durable cache. Use a cron or Cloud Run job to prune temp_synthesis_output if you retain disk between deployments. 6. Health Checks \u00b6 Use GET / for a lightweight ping. To simulate the metadata flow without a real upload, POST to /synthesize with dataset_name=debug_dataset (returns stub metadata). Refer to docs/testing.md for CI-friendly commands to verify the deployment image before shipping.","title":"Deployment"},{"location":"deployment/#deployment-guide","text":"This document outlines the most common deployment paths for the PrivSyn web application\u2014local Docker usage, Google Cloud Run, and Vercel (frontend). It highlights the environment variables you need to set and the expected directory layout.","title":"Deployment Guide"},{"location":"deployment/#1-local-docker-image","text":"Build the image: bash docker build -t privsyn-tabular . Run the container, exposing the FastAPI port: bash docker run --rm -p 8080:8080 \\ -e VITE_API_BASE_URL=\"http://localhost:8080\" \\ privsyn-tabular The backend listens on $PORT (defaults to 8080 for Cloud Run compatibility). The Dockerfile bundles the built frontend under web_app/static/ , so no additional proxy is required.","title":"1. Local Docker Image"},{"location":"deployment/#2-google-cloud-run","text":"Build and push the container: bash gcloud builds submit --tag gcr.io/<PROJECT_ID>/privsyn Deploy to Cloud Run: bash gcloud run deploy privsyn \\ --image gcr.io/<PROJECT_ID>/privsyn \\ --platform managed \\ --allow-unauthenticated \\ --set-env-vars=\"VITE_API_BASE_URL=https://<service-url>\" Configure Cloud Run service variables: VITE_API_BASE_URL : the public URL of the service if you intend to serve the frontend from the same container. CORS_ALLOW_ORIGINS : optional comma-separated list of additional origins to append to the defaults in web_app/main.py . Free tier constraints: the Cloud Run free tier grants limited CPU, RAM, and request duration. Large uploads or AIM runs frequently exceed those caps, leading to timeouts or OOM restarts. For heavy workloads, pull the code locally (or to a beefier VM) and run the backend outside Cloud Run.","title":"2. Google Cloud Run"},{"location":"deployment/#21-continuous-deployment-from-github-actions","text":"Automated deployments run via .github/workflows/deploy-cloudrun.yml whenever main is updated. The workflow: Checks out the repository. Authenticates to GCP using GCP_SA_KEY (a JSON service-account key stored as a GitHub secret). Builds and pushes gcr.io/gen-lang-client-0649776758/privsyn-tabular via gcloud builds submit . Deploys the image to Cloud Run in us-east4 with --allow-unauthenticated . To keep it working you must ensure the following GitHub secrets are defined: Secret Value GCP_SA_KEY Service-account JSON with roles Cloud Run Admin , Cloud Build Editor , Service Account User . The project ( gen-lang-client-0649776758 ), region ( us-east4 ), and image name are baked into the workflow. If you need a different target, update .github/workflows/deploy-cloudrun.yml .","title":"2.1 Continuous deployment from GitHub Actions"},{"location":"deployment/#3-vercel-frontend-hosted-backend","text":"Deploy the backend (Docker, Cloud Run, or elsewhere) and note the public base URL. In the Vercel project (or other static hosting provider): Set VITE_API_BASE_URL to the backend URL. Run npm run build to produce frontend/dist . Serve the built assets or configure Vercel to use the static output directory. Update allow_origins in web_app/main.py to include the Vercel domain.","title":"3. Vercel Frontend + Hosted Backend"},{"location":"deployment/#4-environment-variables","text":"Variable Purpose Default VITE_API_BASE_URL Frontend \u2192 backend URL. http://127.0.0.1:8001 CORS_ALLOW_ORIGINS Optional extra origins; comma separated. \u2014 PORT FastAPI listening port (Cloud Run). 8080 LOG_LEVEL Optional override for FastAPI logging level. INFO","title":"4. Environment Variables"},{"location":"deployment/#5-storage-sessions","text":"Temporary artifacts (uploaded parquet files, synthesized CSVs) are stored under temp_synthesis_output/runs/{session_id} . Sessions expire automatically after six hours. For production, consider pointing the session store to Redis or another durable cache. Use a cron or Cloud Run job to prune temp_synthesis_output if you retain disk between deployments.","title":"5. Storage &amp; Sessions"},{"location":"deployment/#6-health-checks","text":"Use GET / for a lightweight ping. To simulate the metadata flow without a real upload, POST to /synthesize with dataset_name=debug_dataset (returns stub metadata). Refer to docs/testing.md for CI-friendly commands to verify the deployment image before shipping.","title":"6. Health Checks"},{"location":"docs-map/","text":"Documentation Overview \u00b6 This folder complements the main repository README with deeper dives into the UI, backend services, and testing strategy. Use the quick links below to jump to the topic you need: frontend.md : Web UI architecture, key components, and validation behaviour. backend.md : FastAPI endpoints, synthesis service plumbing, and metadata flow. testing.md : Pytest organisation, Playwright end-to-end checks, and recommended quick commands. deployment.md : Docker, Cloud Run, and Vercel deployment tips plus required environment variables. privacy.md : High-level guidance on epsilon/delta, rho-CDP, and how each synthesiser consumes the budget. media/ : Sequence diagrams and flow charts used by the README. sample_output/ : Example synthetic CSV and evaluation metrics to preview the final artefacts. Want to add more detail? Create a new Markdown file under docs/ and cross-link it here and in the main README.","title":"Documentation Map"},{"location":"docs-map/#documentation-overview","text":"This folder complements the main repository README with deeper dives into the UI, backend services, and testing strategy. Use the quick links below to jump to the topic you need: frontend.md : Web UI architecture, key components, and validation behaviour. backend.md : FastAPI endpoints, synthesis service plumbing, and metadata flow. testing.md : Pytest organisation, Playwright end-to-end checks, and recommended quick commands. deployment.md : Docker, Cloud Run, and Vercel deployment tips plus required environment variables. privacy.md : High-level guidance on epsilon/delta, rho-CDP, and how each synthesiser consumes the budget. media/ : Sequence diagrams and flow charts used by the README. sample_output/ : Example synthetic CSV and evaluation metrics to preview the final artefacts. Want to add more detail? Create a new Markdown file under docs/ and cross-link it here and in the main README.","title":"Documentation Overview"},{"location":"frontend/","text":"Frontend Guide \u00b6 High-Level Flow \u00b6 Upload Form ( SynthesisForm.jsx ) Validates file type ( .csv / .zip ), ensures consistent row lengths, and displays inline errors. Supports loading the sample dataset without a file. Metadata Confirmation ( MetadataConfirmation.jsx ) Normalises inferred domain data so categorical/numerical editors always have sensible defaults. Validation rules (as of now): Categorical columns must retain at least one category (selected or custom). Numerical columns require both min and max and enforce max \u2265 min . Shows aggregated errors before allowing submission. Results ( ResultsDisplay.jsx ) Displays download link, preview table, and evaluation JSON payloads. These steps mirror the Playwright E2E scenario in test/e2e/test_frontend_backend_e2e.py . When evolving the UI, align component changes with assertions there to keep regressions visible. Components at a Glance \u00b6 Component Purpose Notes SynthesisForm.jsx Handles dataset name, epsilon/delta, method selection, and file upload. Relies on PapaParse for CSV validation; errors bubble up via local state. MetadataConfirmation.jsx Top-level container for the confirmation card, orchestrates per-column editors and validation. Tracks validation issues in state; any red banner you see is triggered here. CategoricalEditor.jsx Lets users select/deselect categories, add custom values, and choose handling for excluded values. Falls back to value_counts , categories_preview , etc., when the inferred list is empty, and blocks duplicate custom entries case-insensitively. NumericalEditor.jsx Edits min/max bounds, binning strategies, and DP budget sliders. Non-numeric warnings show when the column couldn\u2019t be parsed numerically. ResultsDisplay.jsx Presents the download button, preview table, and evaluation JSON block. Shows inline warnings when evaluation fails and lets the user retry; download link is disabled while evaluation is in flight. Component Tests \u00b6 MetadataConfirmation.test.jsx (Vitest) confirms that categorical lists render even when the backend only supplies value_counts , and it verifies the red alert banner blocks submission when categories are cleared or numeric bounds invert. NumericalEditor.test.jsx (Vitest) exercises the per-bound validation warnings (e.g., extremely large magnitudes and bad numeric input). Add further tests alongside their components in frontend/src/components/ \u2014Vitest picks up *.test.jsx files automatically. Manual Testing Checklist \u00b6 Upload a well-formed CSV and confirm that the metadata table is populated. Try an invalid CSV (e.g., mismatched columns) and ensure the form surfaces the error. On the confirmation page, clear all categories for a column and assert the validation banner appears; fix the error and verify submission succeeds. Enter an extremely large minimum/maximum (e.g., 1e12 ) and verify the inline warning appears; try an invalid string to confirm it is rejected without updating the value. Attempt to add a custom category whose value already exists (e.g., female vs Female ) and confirm the inline warning appears. Check number bounds (blank, non-numeric, max < min) all trigger errors. Confirm the results page renders both the preview and the evaluation JSON. Useful Commands \u00b6 # Start the dev server cd frontend npm install npm run dev -- --port 5174 # Lint npm run lint # Frontend end-to-end test (requires backend running) E2E=1 pytest -q -k end_to_end","title":"Frontend"},{"location":"frontend/#frontend-guide","text":"","title":"Frontend Guide"},{"location":"frontend/#high-level-flow","text":"Upload Form ( SynthesisForm.jsx ) Validates file type ( .csv / .zip ), ensures consistent row lengths, and displays inline errors. Supports loading the sample dataset without a file. Metadata Confirmation ( MetadataConfirmation.jsx ) Normalises inferred domain data so categorical/numerical editors always have sensible defaults. Validation rules (as of now): Categorical columns must retain at least one category (selected or custom). Numerical columns require both min and max and enforce max \u2265 min . Shows aggregated errors before allowing submission. Results ( ResultsDisplay.jsx ) Displays download link, preview table, and evaluation JSON payloads. These steps mirror the Playwright E2E scenario in test/e2e/test_frontend_backend_e2e.py . When evolving the UI, align component changes with assertions there to keep regressions visible.","title":"High-Level Flow"},{"location":"frontend/#components-at-a-glance","text":"Component Purpose Notes SynthesisForm.jsx Handles dataset name, epsilon/delta, method selection, and file upload. Relies on PapaParse for CSV validation; errors bubble up via local state. MetadataConfirmation.jsx Top-level container for the confirmation card, orchestrates per-column editors and validation. Tracks validation issues in state; any red banner you see is triggered here. CategoricalEditor.jsx Lets users select/deselect categories, add custom values, and choose handling for excluded values. Falls back to value_counts , categories_preview , etc., when the inferred list is empty, and blocks duplicate custom entries case-insensitively. NumericalEditor.jsx Edits min/max bounds, binning strategies, and DP budget sliders. Non-numeric warnings show when the column couldn\u2019t be parsed numerically. ResultsDisplay.jsx Presents the download button, preview table, and evaluation JSON block. Shows inline warnings when evaluation fails and lets the user retry; download link is disabled while evaluation is in flight.","title":"Components at a Glance"},{"location":"frontend/#component-tests","text":"MetadataConfirmation.test.jsx (Vitest) confirms that categorical lists render even when the backend only supplies value_counts , and it verifies the red alert banner blocks submission when categories are cleared or numeric bounds invert. NumericalEditor.test.jsx (Vitest) exercises the per-bound validation warnings (e.g., extremely large magnitudes and bad numeric input). Add further tests alongside their components in frontend/src/components/ \u2014Vitest picks up *.test.jsx files automatically.","title":"Component Tests"},{"location":"frontend/#manual-testing-checklist","text":"Upload a well-formed CSV and confirm that the metadata table is populated. Try an invalid CSV (e.g., mismatched columns) and ensure the form surfaces the error. On the confirmation page, clear all categories for a column and assert the validation banner appears; fix the error and verify submission succeeds. Enter an extremely large minimum/maximum (e.g., 1e12 ) and verify the inline warning appears; try an invalid string to confirm it is rejected without updating the value. Attempt to add a custom category whose value already exists (e.g., female vs Female ) and confirm the inline warning appears. Check number bounds (blank, non-numeric, max < min) all trigger errors. Confirm the results page renders both the preview and the evaluation JSON.","title":"Manual Testing Checklist"},{"location":"frontend/#useful-commands","text":"# Start the dev server cd frontend npm install npm run dev -- --port 5174 # Lint npm run lint # Frontend end-to-end test (requires backend running) E2E=1 pytest -q -k end_to_end","title":"Useful Commands"},{"location":"privacy/","text":"Privacy Budget Notes \u00b6 PrivSyn supports two synthesis engines\u2014PrivSyn (rho-CDP) and AIM (approximate zCDP). This note summarises how epsilon, delta, and rho play together so you can rationalise the budget before deploying to production. 1. PrivSyn \u00b6 Interface : the API accepts an epsilon / delta pair. Internally the PrivSyn library converts these to a rho-zCDP budget using method/util/rho_cdp.py . Iterations : the number of update iterations ( update_iterations ) and consistency passes ( consist_iterations ) consume the same global budget. Reducing them lowers overall runtime and the amount of per-marginal noise but also reduces fidelity. Preprocessing : discretisation (e.g. PrivTree binning) can optionally draw a portion of the budget via dp_budget_fraction in the UI. When set, the fraction reduces the budget available to the main synthesiser. Practical Tips \u00b6 Prefer providing tight numeric bounds to avoid the extra noise introduced by clipping. For categorical columns, trimming rare values (or mapping them to the special token) improves stability. When exploring, start with \u03b5 between 0.5 and 2.0 and \u03b4 \u2264 1e-5; adjust upward only if the quality is insufficient and privacy policy allows it. 2. AIM \u00b6 AIM consumes the provided (epsilon, delta) pair directly. Runtime is sensitive to the number of iterations and workload size; for quick evaluations consider reducing the workload prior to deployment or sampling a subset of columns. 3. Rho Conversion \u00b6 The helper method/util/rho_cdp.py implements conversions between (\u03b5, \u03b4) and \u03c1 (zCDP). Keep in mind that when composing multiple DP mechanisms (e.g., preprocessing + synthesis + evaluation), the rho values add linearly: \u03c1_total = \u03c1_preprocess + \u03c1_synthesis + \u03c1_evaluation Use the conversion utilities if you need to enforce a global privacy ledger across multiple pipelines. 4. Evaluation Considerations \u00b6 The /evaluate endpoint uses histogram-aware TVD which is not differentially private\u2014it's intended only for offline quality assessment. Avoid exposing it to untrusted users or run it only on subsampled/aggregated outputs. 5. Future Work \u00b6 Pluggable privacy budget managers (e.g., Google DP accounting) to track consumption across multiple runs. UI hints for typical epsilon/delta values per domain (health, finance, etc.). For more background on CDP and the PrivSyn method, see the references in the pdf/ folder.","title":"Privacy"},{"location":"privacy/#privacy-budget-notes","text":"PrivSyn supports two synthesis engines\u2014PrivSyn (rho-CDP) and AIM (approximate zCDP). This note summarises how epsilon, delta, and rho play together so you can rationalise the budget before deploying to production.","title":"Privacy Budget Notes"},{"location":"privacy/#1-privsyn","text":"Interface : the API accepts an epsilon / delta pair. Internally the PrivSyn library converts these to a rho-zCDP budget using method/util/rho_cdp.py . Iterations : the number of update iterations ( update_iterations ) and consistency passes ( consist_iterations ) consume the same global budget. Reducing them lowers overall runtime and the amount of per-marginal noise but also reduces fidelity. Preprocessing : discretisation (e.g. PrivTree binning) can optionally draw a portion of the budget via dp_budget_fraction in the UI. When set, the fraction reduces the budget available to the main synthesiser.","title":"1. PrivSyn"},{"location":"privacy/#practical-tips","text":"Prefer providing tight numeric bounds to avoid the extra noise introduced by clipping. For categorical columns, trimming rare values (or mapping them to the special token) improves stability. When exploring, start with \u03b5 between 0.5 and 2.0 and \u03b4 \u2264 1e-5; adjust upward only if the quality is insufficient and privacy policy allows it.","title":"Practical Tips"},{"location":"privacy/#2-aim","text":"AIM consumes the provided (epsilon, delta) pair directly. Runtime is sensitive to the number of iterations and workload size; for quick evaluations consider reducing the workload prior to deployment or sampling a subset of columns.","title":"2. AIM"},{"location":"privacy/#3-rho-conversion","text":"The helper method/util/rho_cdp.py implements conversions between (\u03b5, \u03b4) and \u03c1 (zCDP). Keep in mind that when composing multiple DP mechanisms (e.g., preprocessing + synthesis + evaluation), the rho values add linearly: \u03c1_total = \u03c1_preprocess + \u03c1_synthesis + \u03c1_evaluation Use the conversion utilities if you need to enforce a global privacy ledger across multiple pipelines.","title":"3. Rho Conversion"},{"location":"privacy/#4-evaluation-considerations","text":"The /evaluate endpoint uses histogram-aware TVD which is not differentially private\u2014it's intended only for offline quality assessment. Avoid exposing it to untrusted users or run it only on subsampled/aggregated outputs.","title":"4. Evaluation Considerations"},{"location":"privacy/#5-future-work","text":"Pluggable privacy budget managers (e.g., Google DP accounting) to track consumption across multiple runs. UI hints for typical epsilon/delta values per domain (health, finance, etc.). For more background on CDP and the PrivSyn method, see the references in the pdf/ folder.","title":"5. Future Work"},{"location":"testing/","text":"Testing Guide \u00b6 Setup \u00b6 Install test dependencies once per virtual environment: python3 -m pip install -r requirements.txt pytest-cov npm install --prefix frontend python3 -m playwright install pytest-cov is optional locally but required for commands that collect coverage (CI installs it automatically). Test Layers \u00b6 Layer Path Description Unit / Integration test/test_*.py Covers API contracts, PrivSyn helpers, preprocessors, etc. Frontend unit frontend/src/**/*.test.jsx Vitest + React Testing Library exercises UI validation (e.g., MetadataConfirmation ). End-to-End test/e2e/ Playwright flows that boot both frontend and backend. Frontend lint frontend/eslint Run via npm run lint . Markers & conventions: - slow : long-running algorithmic tests. Skip with -m \"not slow\" for fast feedback. - e2e : Playwright browser automation. Enable with E2E=1 to avoid booting browsers by default. - -W error::DeprecationWarning -W error::FutureWarning : promote warnings to failures on critical packages when hardening a change. Quick Commands \u00b6 # Fast unit/integration loop pytest -q -m \"not slow\" # Full suite with coverage (requires pytest-cov) pytest --cov=. --cov-report=term # Warning hygiene for backend + core helpers pytest -q -W error::DeprecationWarning -W error::FutureWarning -k \"web_app or method/preprocess_common or method/synthesis/privsyn\" # Run only API contract tests pytest -q test/test_api_contract.py # Frontend component tests cd frontend npm test -- --run # Playwright end-to-end (requires Playwright browsers & npm deps) E2E=1 pytest -q -k e2e What\u2019s Covered? \u00b6 Metadata overrides ( test/test_metadata_overrides.py ): round-trip of /synthesize \u2192 /confirm_synthesis , categorical resample strategy, numeric coercion, and exponential binning scenarios. PrivSyn internals ( test/test_privsyn_* ): domain helpers, update config, records update, marginal selection. Discretizers ( test/test_privtree.py , test/test_dawa.py ): deterministic unit tests for PrivTree and DAWA helpers. API contracts ( test/test_api*.py ): unified synthesizer interface, metrics hook, deterministic sampling semantics. End-to-end ( test/e2e/test_frontend_backend_e2e.py ): upload sample \u2192 confirm metadata (category fallback + validation) \u2192 synthesize \u2192 download. Frontend components ( frontend/src/components/*.test.jsx ): Vitest suites covering metadata confirmation, categorical editing, and numeric bound validation warnings. Local Tips \u00b6 Install Playwright browsers once with python3 -m playwright install --with-deps if you plan to run E2E. Ensure ports 8001 (backend) and 5174 (frontend) are free before running E2E=1 tests; the suite spawns both servers automatically. Browser tests default to Chromium; you can change p.chromium.launch() to other browsers if needed. Set PLAYWRIGHT_HEADLESS=0 to watch the browser session while debugging. CI Notes \u00b6 GitHub Actions workflow ci.yml runs the backend tests with coverage and uploads to Codecov when CODECOV_TOKEN is configured. E2E job ( jobs.e2e ) is opt-in\u2014triggered by setting E2E=1 pytest -q -k e2e after installing Playwright dependencies.","title":"Testing"},{"location":"testing/#testing-guide","text":"","title":"Testing Guide"},{"location":"testing/#setup","text":"Install test dependencies once per virtual environment: python3 -m pip install -r requirements.txt pytest-cov npm install --prefix frontend python3 -m playwright install pytest-cov is optional locally but required for commands that collect coverage (CI installs it automatically).","title":"Setup"},{"location":"testing/#test-layers","text":"Layer Path Description Unit / Integration test/test_*.py Covers API contracts, PrivSyn helpers, preprocessors, etc. Frontend unit frontend/src/**/*.test.jsx Vitest + React Testing Library exercises UI validation (e.g., MetadataConfirmation ). End-to-End test/e2e/ Playwright flows that boot both frontend and backend. Frontend lint frontend/eslint Run via npm run lint . Markers & conventions: - slow : long-running algorithmic tests. Skip with -m \"not slow\" for fast feedback. - e2e : Playwright browser automation. Enable with E2E=1 to avoid booting browsers by default. - -W error::DeprecationWarning -W error::FutureWarning : promote warnings to failures on critical packages when hardening a change.","title":"Test Layers"},{"location":"testing/#quick-commands","text":"# Fast unit/integration loop pytest -q -m \"not slow\" # Full suite with coverage (requires pytest-cov) pytest --cov=. --cov-report=term # Warning hygiene for backend + core helpers pytest -q -W error::DeprecationWarning -W error::FutureWarning -k \"web_app or method/preprocess_common or method/synthesis/privsyn\" # Run only API contract tests pytest -q test/test_api_contract.py # Frontend component tests cd frontend npm test -- --run # Playwright end-to-end (requires Playwright browsers & npm deps) E2E=1 pytest -q -k e2e","title":"Quick Commands"},{"location":"testing/#whats-covered","text":"Metadata overrides ( test/test_metadata_overrides.py ): round-trip of /synthesize \u2192 /confirm_synthesis , categorical resample strategy, numeric coercion, and exponential binning scenarios. PrivSyn internals ( test/test_privsyn_* ): domain helpers, update config, records update, marginal selection. Discretizers ( test/test_privtree.py , test/test_dawa.py ): deterministic unit tests for PrivTree and DAWA helpers. API contracts ( test/test_api*.py ): unified synthesizer interface, metrics hook, deterministic sampling semantics. End-to-end ( test/e2e/test_frontend_backend_e2e.py ): upload sample \u2192 confirm metadata (category fallback + validation) \u2192 synthesize \u2192 download. Frontend components ( frontend/src/components/*.test.jsx ): Vitest suites covering metadata confirmation, categorical editing, and numeric bound validation warnings.","title":"What\u2019s Covered?"},{"location":"testing/#local-tips","text":"Install Playwright browsers once with python3 -m playwright install --with-deps if you plan to run E2E. Ensure ports 8001 (backend) and 5174 (frontend) are free before running E2E=1 tests; the suite spawns both servers automatically. Browser tests default to Chromium; you can change p.chromium.launch() to other browsers if needed. Set PLAYWRIGHT_HEADLESS=0 to watch the browser session while debugging.","title":"Local Tips"},{"location":"testing/#ci-notes","text":"GitHub Actions workflow ci.yml runs the backend tests with coverage and uploads to Codecov when CODECOV_TOKEN is configured. E2E job ( jobs.e2e ) is opt-in\u2014triggered by setting E2E=1 pytest -q -k e2e after installing Playwright dependencies.","title":"CI Notes"}]}